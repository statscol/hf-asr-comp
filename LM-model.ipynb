{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c629a67500d04ee0985bb19075880d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.8ubuntu1.1).\n",
      "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu1.2).\n",
      "zlib1g-dev set to manually installed.\n",
      "The following additional packages will be installed:\n",
      "  bzip2-doc cmake-data libarchive13 libboost-atomic1.71-dev\n",
      "  libboost-atomic1.71.0 libboost-chrono1.71-dev libboost-chrono1.71.0\n",
      "  libboost-date-time1.71-dev libboost-date-time1.71.0\n",
      "  libboost-program-options1.71-dev libboost-program-options1.71.0\n",
      "  libboost-serialization1.71-dev libboost-serialization1.71.0\n",
      "  libboost-system1.71-dev libboost-system1.71.0 libboost-test1.71-dev\n",
      "  libboost-test1.71.0 libboost-thread1.71-dev libboost-thread1.71.0\n",
      "  libboost1.71-dev libglib2.0-0 libglib2.0-data libicu66 libjsoncpp1 librhash0\n",
      "  libuv1 libxml2 pkg-config shared-mime-info tzdata xdg-user-dirs\n",
      "Suggested packages:\n",
      "  cmake-doc ninja-build lrzip libboost1.71-doc libboost-container1.71-dev\n",
      "  libboost-context1.71-dev libboost-contract1.71-dev\n",
      "  libboost-coroutine1.71-dev libboost-exception1.71-dev libboost-fiber1.71-dev\n",
      "  libboost-filesystem1.71-dev libboost-graph1.71-dev\n",
      "  libboost-graph-parallel1.71-dev libboost-iostreams1.71-dev\n",
      "  libboost-locale1.71-dev libboost-log1.71-dev libboost-math1.71-dev\n",
      "  libboost-mpi1.71-dev libboost-mpi-python1.71-dev libboost-numpy1.71-dev\n",
      "  libboost-python1.71-dev libboost-random1.71-dev libboost-regex1.71-dev\n",
      "  libboost-stacktrace1.71-dev libboost-timer1.71-dev\n",
      "  libboost-type-erasure1.71-dev libboost-wave1.71-dev libboost1.71-tools-dev\n",
      "  libmpfrc++-dev libntl-dev libeigen3-doc liblzma-doc\n",
      "The following NEW packages will be installed:\n",
      "  bzip2-doc cmake cmake-data libarchive13 libboost-atomic1.71-dev\n",
      "  libboost-atomic1.71.0 libboost-chrono1.71-dev libboost-chrono1.71.0\n",
      "  libboost-date-time1.71-dev libboost-date-time1.71.0\n",
      "  libboost-program-options-dev libboost-program-options1.71-dev\n",
      "  libboost-program-options1.71.0 libboost-serialization1.71-dev\n",
      "  libboost-serialization1.71.0 libboost-system-dev libboost-system1.71-dev\n",
      "  libboost-system1.71.0 libboost-test-dev libboost-test1.71-dev\n",
      "  libboost-test1.71.0 libboost-thread-dev libboost-thread1.71-dev\n",
      "  libboost-thread1.71.0 libboost1.71-dev libbz2-dev libeigen3-dev libglib2.0-0\n",
      "  libglib2.0-data libicu66 libjsoncpp1 liblzma-dev librhash0 libuv1 libxml2\n",
      "  pkg-config shared-mime-info tzdata xdg-user-dirs\n",
      "0 upgraded, 39 newly installed, 0 to remove and 80 not upgraded.\n",
      "Need to get 32.2 MB of archives.\n",
      "After this operation, 268 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglib2.0-0 amd64 2.64.6-1~ubuntu20.04.4 [1287 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglib2.0-data all 2.64.6-1~ubuntu20.04.4 [6052 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 tzdata all 2021e-0ubuntu0.20.04 [295 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libicu66 amd64 66.1-2ubuntu2.1 [8515 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libxml2 amd64 2.9.10+dfsg-5ubuntu0.20.04.1 [640 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 shared-mime-info amd64 1.15-1 [430 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 xdg-user-dirs amd64 0.17-2ubuntu1 [48.3 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libuv1 amd64 1.34.2-1ubuntu1.3 [80.8 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 bzip2-doc all 1.0.8-2 [501 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 cmake-data all 3.16.3-1ubuntu1 [1612 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libarchive13 amd64 3.4.0-2ubuntu1 [327 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libjsoncpp1 amd64 1.7.4-3.1ubuntu2 [75.6 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 librhash0 amd64 1.3.9-1 [113 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 cmake amd64 3.16.3-1ubuntu1 [3669 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost1.71-dev amd64 1.71.0-6ubuntu6 [9068 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-atomic1.71.0 amd64 1.71.0-6ubuntu6 [205 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-atomic1.71-dev amd64 1.71.0-6ubuntu6 [205 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-chrono1.71.0 amd64 1.71.0-6ubuntu6 [217 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-chrono1.71-dev amd64 1.71.0-6ubuntu6 [225 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-date-time1.71.0 amd64 1.71.0-6ubuntu6 [219 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-serialization1.71.0 amd64 1.71.0-6ubuntu6 [302 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-serialization1.71-dev amd64 1.71.0-6ubuntu6 [344 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-date-time1.71-dev amd64 1.71.0-6ubuntu6 [228 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-program-options1.71.0 amd64 1.71.0-6ubuntu6 [342 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-program-options1.71-dev amd64 1.71.0-6ubuntu6 [374 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-program-options-dev amd64 1.71.0.0ubuntu2 [3408 B]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-system1.71.0 amd64 1.71.0-6ubuntu6 [205 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-system1.71-dev amd64 1.71.0-6ubuntu6 [205 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-system-dev amd64 1.71.0.0ubuntu2 [3536 B]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu focal/universe amd64 libboost-test1.71.0 amd64 1.71.0-6ubuntu6 [437 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu focal/universe amd64 libboost-test1.71-dev amd64 1.71.0-6ubuntu6 [513 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu focal/universe amd64 libboost-test-dev amd64 1.71.0.0ubuntu2 [3424 B]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-thread1.71.0 amd64 1.71.0-6ubuntu6 [249 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-thread1.71-dev amd64 1.71.0-6ubuntu6 [258 kB]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu focal/main amd64 libboost-thread-dev amd64 1.71.0.0ubuntu2 [3416 B]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu focal/main amd64 libbz2-dev amd64 1.0.8-2 [30.2 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu focal/main amd64 pkg-config amd64 0.29.1-0ubuntu4 [45.5 kB]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu focal/universe amd64 libeigen3-dev all 3.3.7-2 [815 kB]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 liblzma-dev amd64 5.2.4-1ubuntu1 [147 kB]\n",
      "Fetched 32.2 MB in 6s (5080 kB/s)                                              \u001b[0m\u001b[33m\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 39.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Extracting templates from packages: 100%\n",
      "Preconfiguring packages ...\n",
      "Configuring tzdata\n",
      "------------------\n",
      "\n",
      "Please select the geographic area in which you live. Subsequent configuration\n",
      "questions will narrow this down by presenting a list of cities, representing\n",
      "the time zones in which they are located.\n",
      "\n",
      "  1. Africa      4. Australia  7. Atlantic  10. Pacific  13. Etc\n",
      "  2. America     5. Arctic     8. Europe    11. SystemV\n",
      "  3. Antarctica  6. Asia       9. Indian    12. US\n",
      "\u001b[4mGeographic area: \u001b[m\u001b[1m^C\n",
      "\u001b[1;31mE: \u001b[0mSub-process /usr/sbin/dpkg-preconfigure --apt || true received signal 2.\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mFailure running script /usr/sbin/dpkg-preconfigure --apt || true\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##requirements to run in terminal\n",
    "!apt install -y build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in terminal\n",
    "!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset spanish_billion_words (/home/jhonparra/.cache/huggingface/datasets/spanish_billion_words/corpus/1.1.0/8ba50a854d61199f7d36b4c3f598589a2f8b493a2644b88ce80adb2cebcbc107)\n"
     ]
    }
   ],
   "source": [
    "##read corpus data\n",
    "data_spanish=load_dataset(\"spanish_billion_words\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import clean_batch,homologate_accents\n",
    "\n",
    "column_text=\"text\"\n",
    "\n",
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[column_text])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246e7ad735ed4a37998dff0b18baf554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46925295 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d692ed4b464ecab82a5bca8cea0ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46925295 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##preprocessing data\n",
    "\n",
    "data_spanish=data_spanish.map(clean_batch,fn_kwargs={\"text_column\":column_text})\n",
    "data_spanish=data_spanish.map(homologate_accents,fn_kwargs={\"text_column\":column_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push dataset to hub\n",
    "\n",
    "Lets generate a checkpoint for processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_lang=\"es\" \n",
    "data_spanish.push_to_hub(f\"spanish_billion_words_clean\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6a98bfcf8c45d388381b6e9e14e2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration jhonparra18--spanish_billion_words_clean-afd4c7aa3941f2eb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset spanish_billion_words/corpus (download: 4.38 GiB, generated: 8.31 GiB, post-processed: Unknown size, total: 12.69 GiB) to /root/.cache/huggingface/datasets/parquet/jhonparra18--spanish_billion_words_clean-afd4c7aa3941f2eb/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3bad5448094817b2e3e30beb340a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebbe8f02db245dbbc93185eedeabc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c538e581c646738486ec9d8df40cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0431951eb04c3a9bc52ae70735d5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/240M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abceaaeea4b4c0981295559de5cbd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/267M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868a3a7fb7954c20a713ecde4569df03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/311M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff402f9434594ac8b108ca43273e1fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/320M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3345eed9faa439691f71f4a2a503c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/322M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0bac05d78d4569a5790de85bb3047a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/320M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20d22b922b245bdb8684c242242d4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/299M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ab9c0ddb544084aa224bfb56bd05c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/305M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c211a0bbcead4346906b0b03697b6815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/299M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2172efdee5d349dab2599c1429205b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877f1bead72146b0b2234664ca0acb79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/238M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d546d07bd0e4c448148682316d1c172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4db5c123064c4e99544bdc9bed56a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4dbeff48d8947d2b83443198edec3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/210M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d084e85564d240e884478cdbac11b8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/196M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b859261ead143f2bfea796446e9dbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/220M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f845a694444747b6997e170136fab33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/jhonparra18--spanish_billion_words_clean-afd4c7aa3941f2eb/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_processed = load_dataset(f\"jhonparra18/spanish_billion_words_clean\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9385059, 1)\n"
     ]
    }
   ],
   "source": [
    "##given memory limitations, a sample dataset must be used\n",
    "dataset_processed_sample=dataset_processed.train_test_split(train_size=0.2)[\"train\"]\n",
    "print(dataset_processed_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text.txt\", \"w\") as file:\n",
    "  file.write(\" \".join(dataset_processed_sample[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '5gram.arpa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8af84524e9e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##after generating .arpa file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"5gram.arpa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"5gram_correct.arpa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwrite_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mhas_added_eos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_added_eos\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"ngram 1=\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '5gram.arpa'"
     ]
    }
   ],
   "source": [
    "##after generating .arpa file\n",
    "with open(\"5gram.arpa\", \"r\") as read_file, open(\"5gram_correct.arpa\", \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor,Wav2Vec2Processor,Wav2Vec2ForCTC\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"jhonparra18/wav2vec2-large-xls-r-300m-spanish-custom\")\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"jhonparra18/wav2vec2-large-xls-r-300m-spanish-custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"jhonparra18/wav2vec2-large-xls-r-300m-spanish-custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "  Downloading https://github.com/kpu/kenlm/archive/master.zip (541 kB)\n",
      "\u001b[K     |████████████████████████████████| 541 kB 44 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pyctcdecode\n",
      "  Downloading pyctcdecode-0.3.0-py2.py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pygtrie<3.0,>=2.1\n",
      "  Downloading pygtrie-2.4.2.tar.gz (35 kB)\n",
      "Collecting hypothesis<7,>=6.14\n",
      "  Downloading hypothesis-6.36.0-py3-none-any.whl (376 kB)\n",
      "\u001b[K     |████████████████████████████████| 376 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.15.0 in /home/jhon.parra/.local/lib/python3.8/site-packages (from pyctcdecode) (1.19.5)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /home/jhon.parra/.local/lib/python3.8/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/jhon.parra/.local/lib/python3.8/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (21.2.0)\n",
      "Building wheels for collected packages: kenlm, pygtrie\n",
      "  Building wheel for kenlm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kenlm: filename=kenlm-0.0.0-cp38-cp38-linux_x86_64.whl size=2977431 sha256=568a80e27bb5be960de83d2231c08e4c8ec40cc5eaf0d8e3df9686209643b0f6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fo8rpfhr/wheels/ff/08/4e/a3ddc0e786e0f3c1fcd2e7a82c4324c02fc3ae2638471406d2\n",
      "  Building wheel for pygtrie (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pygtrie: filename=pygtrie-2.4.2-py3-none-any.whl size=19062 sha256=f26467546cd05e7f9b8ffc7555029c7a372022f58b86e4aef7dc7d206e9f4f31\n",
      "  Stored in directory: /home/jhon.parra/.cache/pip/wheels/31/03/7b/f685b394a937bc97d2d40908d45aa31f3d9473bca6e9019153\n",
      "Successfully built kenlm pygtrie\n",
      "Installing collected packages: pygtrie, hypothesis, pyctcdecode, kenlm\n",
      "Successfully installed hypothesis-6.36.0 kenlm-0.0.0 pyctcdecode-0.3.0 pygtrie-2.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/kpu/kenlm/archive/master.zip pyctcdecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unigrams not provided and cannot be automatically determined from LM file (only arpa format). Decoding accuracy might be reduced.\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "No known unigrams provided, decoding results might be a lot worse.\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=\"5gram_correct.arpa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##install git-lfs\n",
    "!sudo apt-get install git-lfs tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##clone repo from hf\n",
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository(local_dir=\"hf-asr-comp\", clone_from=\"jhonparra18/wav2vec2-large-xls-r-300m-spanish-custom\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save model\n",
    "processor_with_lm.save_pretrained(\"hf-asr-comp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##in case 5gram file is too big, convert it to binary using the following in the command line\n",
    "!kenlm/build/bin/build_binary hf-asr-comp/language_model/5gram_correct.arpa hf-asr-comp/language_model/5gram.bin\n",
    "##then delete old file to avoid it from being added to the repo\n",
    "!rm hf-asr-comp/language_model/5gram_correct.arpa && tree -h hf-asr-comp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['language_model/unigrams.txt']. This may take a bit of time if the files are large.\n",
      "Upload file language_model/5gram.bin: 14.0GB [24:08, 12.3MB/s]                            To https://huggingface.co/jhonparra18/wav2vec2-large-xls-r-300m-spanish-custom\n",
      "   ef92bd3..90f6ce6  main -> main\n",
      "\n",
      "Upload file language_model/5gram.bin: 100%|██████████| 9.68G/9.68G [24:10<00:00, 7.17MB/s]\n",
      "Upload file language_model/unigrams.txt: 100%|██████████| 12.5M/12.5M [24:10<00:00, 9.03kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/jhonparra18/wav2vec2-large-xls-r-300m-spanish-custom/commit/90f6ce68df655dd2ebff32d542c733506c2dad13'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.push_to_hub(commit_message=\"LM added using 5-gram model\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
