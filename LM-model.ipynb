{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9799b17cf1e642a0a9a16e8b323af79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##requirements to run in terminal\n",
    "!sudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset spanish_billion_words (/home/jhonparra/.cache/huggingface/datasets/spanish_billion_words/corpus/1.1.0/8ba50a854d61199f7d36b4c3f598589a2f8b493a2644b88ce80adb2cebcbc107)\n"
     ]
    }
   ],
   "source": [
    "##read corpus data\n",
    "data_spanish=load_dataset(\"spanish_billion_words\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import clean_batch,homologate_accents\n",
    "\n",
    "column_text=\"text\"\n",
    "\n",
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[column_text])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246e7ad735ed4a37998dff0b18baf554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46925295 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d692ed4b464ecab82a5bca8cea0ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46925295 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##preprocessing data\n",
    "\n",
    "data_spanish=data_spanish.map(clean_batch,fn_kwargs={\"text_column\":column_text})\n",
    "data_spanish=data_spanish.map(homologate_accents,fn_kwargs={\"text_column\":column_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push dataset to hub\n",
    "\n",
    "Lets generate a checkpoint for processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_lang=\"es\" \n",
    "data_spanish.push_to_hub(f\"spanish_billion_words_clean\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration jhonparra18--spanish_billion_words_clean-afd4c7aa3941f2eb\n",
      "Reusing dataset parquet (/home/jhon.parra/.cache/huggingface/datasets/parquet/jhonparra18--spanish_billion_words_clean-afd4c7aa3941f2eb/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_processed = load_dataset(f\"jhonparra18/spanish_billion_words_clean\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4692529, 1)\n"
     ]
    }
   ],
   "source": [
    "##given memory limitations, a sample dataset must be used\n",
    "dataset_processed_sample=dataset_processed.train_test_split(train_size=0.2)[\"train\"]\n",
    "print(dataset_processed_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text.txt\", \"w\") as file:\n",
    "  file.write(\" \".join(dataset_processed_sample[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##after generating .arpa file\n",
    "with open(\"5gram.arpa\", \"r\") as read_file, open(\"5gram_correct.arpa\", \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor,Wav2Vec2Processor,Wav2Vec2ForCTC\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"jhonparra18/wav2vec2-large-xls-r-300m-spanish-custom\")\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"jhonparra18/wav2vec2-large-xls-r-300m-spanish-custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"jhonparra18/wav2vec2-large-xls-r-300m-spanish-custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "  Downloading https://github.com/kpu/kenlm/archive/master.zip (541 kB)\n",
      "\u001b[K     |████████████████████████████████| 541 kB 44 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pyctcdecode\n",
      "  Downloading pyctcdecode-0.3.0-py2.py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pygtrie<3.0,>=2.1\n",
      "  Downloading pygtrie-2.4.2.tar.gz (35 kB)\n",
      "Collecting hypothesis<7,>=6.14\n",
      "  Downloading hypothesis-6.36.0-py3-none-any.whl (376 kB)\n",
      "\u001b[K     |████████████████████████████████| 376 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.15.0 in /home/jhon.parra/.local/lib/python3.8/site-packages (from pyctcdecode) (1.19.5)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /home/jhon.parra/.local/lib/python3.8/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/jhon.parra/.local/lib/python3.8/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (21.2.0)\n",
      "Building wheels for collected packages: kenlm, pygtrie\n",
      "  Building wheel for kenlm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kenlm: filename=kenlm-0.0.0-cp38-cp38-linux_x86_64.whl size=2977431 sha256=568a80e27bb5be960de83d2231c08e4c8ec40cc5eaf0d8e3df9686209643b0f6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fo8rpfhr/wheels/ff/08/4e/a3ddc0e786e0f3c1fcd2e7a82c4324c02fc3ae2638471406d2\n",
      "  Building wheel for pygtrie (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pygtrie: filename=pygtrie-2.4.2-py3-none-any.whl size=19062 sha256=f26467546cd05e7f9b8ffc7555029c7a372022f58b86e4aef7dc7d206e9f4f31\n",
      "  Stored in directory: /home/jhon.parra/.cache/pip/wheels/31/03/7b/f685b394a937bc97d2d40908d45aa31f3d9473bca6e9019153\n",
      "Successfully built kenlm pygtrie\n",
      "Installing collected packages: pygtrie, hypothesis, pyctcdecode, kenlm\n",
      "Successfully installed hypothesis-6.36.0 kenlm-0.0.0 pyctcdecode-0.3.0 pygtrie-2.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/kpu/kenlm/archive/master.zip pyctcdecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unigrams not provided and cannot be automatically determined from LM file (only arpa format). Decoding accuracy might be reduced.\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "No known unigrams provided, decoding results might be a lot worse.\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=\"5gram_correct.arpa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##install git-lfs\n",
    "!sudo apt-get install git-lfs tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##clone repo from hf\n",
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository(local_dir=\"hf-asr-comp\", clone_from=\"jhonparra18/wav2vec2-large-xls-r-300m-spanish-custom\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save model\n",
    "processor_with_lm.save_pretrained(\"hf-asr-comp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##in case 5gram file is too big, convert it to binary using the following in the command line\n",
    "!kenlm/build/bin/build_binary hf-asr-comp/language_model/5gram_correct.arpa hf-asr-comp/language_model/5gram.bin\n",
    "##then delete old file to avoid it from being added to the repo\n",
    "!rm hf-asr-comp/language_model/5gram_correct.arpa && tree -h hf-asr-comp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['language_model/unigrams.txt']. This may take a bit of time if the files are large.\n",
      "Upload file language_model/5gram.bin: 14.0GB [24:08, 12.3MB/s]                            To https://huggingface.co/jhonparra18/wav2vec2-large-xls-r-300m-spanish-custom\n",
      "   ef92bd3..90f6ce6  main -> main\n",
      "\n",
      "Upload file language_model/5gram.bin: 100%|██████████| 9.68G/9.68G [24:10<00:00, 7.17MB/s]\n",
      "Upload file language_model/unigrams.txt: 100%|██████████| 12.5M/12.5M [24:10<00:00, 9.03kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/jhonparra18/wav2vec2-large-xls-r-300m-spanish-custom/commit/90f6ce68df655dd2ebff32d542c733506c2dad13'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.push_to_hub(commit_message=\"LM added using 5-gram model\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
